{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a6023e",
   "metadata": {},
   "source": [
    "# Constitutional Law LLM Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation tools for the trained Constitutional Law LLM.\n",
    "\n",
    "## Overview\n",
    "- Model performance evaluation\n",
    "- Generation quality analysis\n",
    "- Legal reasoning assessment\n",
    "- Comparative analysis across parameters\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b39997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our modules\n",
    "from config import config\n",
    "from model_utils import ModelManager\n",
    "from model_training import ConstitutionalLawTrainer\n",
    "\n",
    "# Add evaluation path\n",
    "sys.path.append('../evaluation')\n",
    "from generation_analysis import evaluate_model, run_generation_analysis, load_test_cases\n",
    "\n",
    "print(\"Evaluation setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8de38",
   "metadata": {},
   "source": [
    "## Load Test Cases\n",
    "\n",
    "Load and inspect the test cases for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases\n",
    "test_file = \"../evaluation/test_cases.json\"\n",
    "test_cases = load_test_cases(test_file)\n",
    "\n",
    "print(f\"Loaded {len(test_cases)} test cases\")\n",
    "\n",
    "# Show test case structure\n",
    "if test_cases:\n",
    "    example = test_cases[0]\n",
    "    print(f\"\\nExample test case:\")\n",
    "    print(f\"ID: {example.get('id', 'N/A')}\")\n",
    "    print(f\"Facts: {example.get('facts', '')[:150]}...\")\n",
    "    print(f\"Question: {example.get('question', '')[:100]}...\")\n",
    "    print(f\"Reference: {example.get('reference', '')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96d6c4",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd432cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model paths to evaluate\n",
    "model_paths = [\n",
    "    \"../models/constitutional_law_trained\",\n",
    "    \"../models/lora_finetuned2\",\n",
    "    \"../models/lora_finetuned3\"\n",
    "]\n",
    "\n",
    "# Check which models exist\n",
    "available_models = []\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        available_models.append(path)\n",
    "        print(f\"✓ Found model: {path}\")\n",
    "    else:\n",
    "        print(f\"✗ Model not found: {path}\")\n",
    "\n",
    "print(f\"\\nEvaluating {len(available_models)} available models...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46418705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each available model\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_path in available_models:\n",
    "    print(f\"\\nEvaluating model: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Run basic evaluation\n",
    "        results = evaluate_model(\n",
    "            model_path=model_path,\n",
    "            test_file=test_file,\n",
    "            base_model_name=config.model.base_model_name\n",
    "        )\n",
    "        \n",
    "        evaluation_results[model_path] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Total cases: {results['summary']['total_cases']}\")\n",
    "        print(f\"  Overall score: {results['summary']['overall_score']:.3f}\")\n",
    "        print(f\"  Average F1: {results['aggregate_metrics'].get('avg_f1', 0):.3f}\")\n",
    "        print(f\"  Average accuracy: {results['aggregate_metrics'].get('avg_accuracy', 0):.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating {model_path}: {e}\")\n",
    "        evaluation_results[model_path] = {\"error\": str(e)}\n",
    "\n",
    "print(f\"\\nEvaluation completed for {len(evaluation_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596619e",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Analyze and visualize the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for model_path, results in evaluation_results.items():\n",
    "    if \"error\" not in results:\n",
    "        model_name = os.path.basename(model_path)\n",
    "        metrics = results['aggregate_metrics']\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Overall Score': results['summary']['overall_score'],\n",
    "            'F1 Score': metrics.get('avg_f1', 0),\n",
    "            'Accuracy': metrics.get('avg_accuracy', 0),\n",
    "            'Legal Term Coverage': metrics.get('avg_legal_term_coverage', 0),\n",
    "            'Constitutional Framework': metrics.get('avg_constitutional_framework', 0),\n",
    "            'Reasoning Structure': metrics.get('avg_reasoning_structure', 0)\n",
    "        })\n",
    "\n",
    "if comparison_data:\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"Model Comparison:\")\n",
    "    print(df_comparison.round(3))\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Overall scores\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(df_comparison['Model'], df_comparison['Overall Score'])\n",
    "    plt.title('Overall Model Performance')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # F1 and Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    x = range(len(df_comparison))\n",
    "    width = 0.35\n",
    "    plt.bar([i - width/2 for i in x], df_comparison['F1 Score'], width, label='F1 Score')\n",
    "    plt.bar([i + width/2 for i in x], df_comparison['Accuracy'], width, label='Accuracy')\n",
    "    plt.title('F1 Score vs Accuracy')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, df_comparison['Model'], rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Legal metrics\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(df_comparison['Model'], df_comparison['Legal Term Coverage'])\n",
    "    plt.title('Legal Term Coverage')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Constitutional framework\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(df_comparison['Model'], df_comparison['Constitutional Framework'])\n",
    "    plt.title('Constitutional Framework Score')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = df_comparison.loc[df_comparison['Overall Score'].idxmax()]\n",
    "    print(f\"\\nBest performing model: {best_model['Model']}\")\n",
    "    print(f\"Overall score: {best_model['Overall Score']:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No successful evaluations to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34d0d2",
   "metadata": {},
   "source": [
    "## Sample Responses\n",
    "\n",
    "Show sample responses from the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd179127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample responses from the best model\n",
    "if comparison_data:\n",
    "    best_model_name = df_comparison.loc[df_comparison['Overall Score'].idxmax(), 'Model']\n",
    "    best_model_path = None\n",
    "    \n",
    "    for path in available_models:\n",
    "        if os.path.basename(path) == best_model_name:\n",
    "            best_model_path = path\n",
    "            break\n",
    "    \n",
    "    if best_model_path:\n",
    "        print(f\"Sample responses from best model: {best_model_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Get detailed results\n",
    "        detailed_results = evaluation_results[best_model_path]['detailed_results']\n",
    "        \n",
    "        # Show first 3 examples\n",
    "        for i, result in enumerate(detailed_results[:3]):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Case ID: {result['case_id']}\")\n",
    "            print(f\"Facts: {result['facts'][:200]}...\")\n",
    "            print(f\"Question: {result['question']}\")\n",
    "            print(f\"Generated Response: {result['generated'][:300]}...\")\n",
    "            print(f\"Reference Response: {result['reference'][:300]}...\")\n",
    "            print(f\"F1 Score: {result['quality_metrics'].get('f1', 0):.3f}\")\n",
    "            print(\"-\" * 60)\n",
    "    else:\n",
    "        print(\"Best model path not found\")\n",
    "else:\n",
    "    print(\"No models to show responses from\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6aad1",
   "metadata": {},
   "source": [
    "## Generation Parameter Analysis\n",
    "\n",
    "Analyze how different generation parameters affect output quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c6bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generation analysis on the best model (if available)\n",
    "if comparison_data and available_models:\n",
    "    best_model_path = None\n",
    "    for path in available_models:\n",
    "        if os.path.basename(path) == best_model_name:\n",
    "            best_model_path = path\n",
    "            break\n",
    "    \n",
    "    if best_model_path:\n",
    "        print(f\"Running generation analysis on: {best_model_name}\")\n",
    "        \n",
    "        # Run with a subset of test cases for faster analysis\n",
    "        subset_cases = test_cases[:5]  # Use first 5 cases\n",
    "        \n",
    "        try:\n",
    "            generation_results = run_generation_analysis(\n",
    "                model_path=best_model_path,\n",
    "                test_file=test_file\n",
    "            )\n",
    "            \n",
    "            print(\"\\nGeneration Analysis Results:\")\n",
    "            print(f\"Total parameter combinations tested: {generation_results['summary']['total_combinations']}\")\n",
    "            print(f\"Best score: {generation_results['summary']['best_score']:.3f}\")\n",
    "            print(f\"Average score: {generation_results['summary']['average_score']:.3f}\")\n",
    "            \n",
    "            print(f\"\\nBest generation parameters:\")\n",
    "            best_params = generation_results['best_params']['params']\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation analysis: {e}\")\n",
    "            print(\"Skipping generation parameter analysis\")\n",
    "    else:\n",
    "        print(\"Best model path not found for generation analysis\")\n",
    "else:\n",
    "    print(\"No models available for generation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634d26b",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Provide summary of evaluation results and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EVALUATION SUMMARY ===\")\n",
    "\n",
    "if comparison_data:\n",
    "    print(f\"Models evaluated: {len(comparison_data)}\")\n",
    "    print(f\"Best model: {best_model['Model']}\")\n",
    "    print(f\"Best overall score: {best_model['Overall Score']:.3f}\")\n",
    "    print(f\"Best F1 score: {best_model['F1 Score']:.3f}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    avg_score = df_comparison['Overall Score'].mean()\n",
    "    print(f\"Average model score: {avg_score:.3f}\")\n",
    "    \n",
    "    if best_model['Overall Score'] > 0.7:\n",
    "        print(\"✓ Strong performance - Model shows good constitutional law reasoning\")\n",
    "    elif best_model['Overall Score'] > 0.5:\n",
    "        print(\"⚠ Moderate performance - Consider additional training or parameter tuning\")\n",
    "    else:\n",
    "        print(\"⚠ Weak performance - Significant improvements needed\")\n",
    "    \n",
    "    print(f\"\\n=== RECOMMENDATIONS ===\")\n",
    "    \n",
    "    # Specific recommendations based on metrics\n",
    "    if best_model['Legal Term Coverage'] < 0.6:\n",
    "        print(\"- Improve legal terminology coverage in training data\")\n",
    "    \n",
    "    if best_model['Constitutional Framework'] < 0.6:\n",
    "        print(\"- Enhance constitutional framework understanding\")\n",
    "    \n",
    "    if best_model['Reasoning Structure'] < 0.6:\n",
    "        print(\"- Improve logical reasoning structure in responses\")\n",
    "    \n",
    "    # General recommendations\n",
    "    print(\"- Consider hyperparameter optimization for better performance\")\n",
    "    print(\"- Evaluate on additional test cases for robustness\")\n",
    "    print(\"- Monitor for bias in constitutional interpretation\")\n",
    "    print(\"- Consider ensemble methods for improved accuracy\")\n",
    "    \n",
    "else:\n",
    "    print(\"No successful evaluations completed\")\n",
    "    print(\"Check model paths and dependencies\")\n",
    "\n",
    "print(f\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Use best model for deployment or further fine-tuning\")\n",
    "print(\"2. Collect additional training data if performance is insufficient\")\n",
    "print(\"3. Implement the recommended generation parameters\")\n",
    "print(\"4. Consider specialized evaluation metrics for legal reasoning\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
