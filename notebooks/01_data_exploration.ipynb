{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4152c128",
   "metadata": {},
   "source": [
    "# Constitutional Law Data Exploration\n",
    "\n",
    "This notebook explores the constitutional law dataset and provides insights into the data structure and quality.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a63f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Data exploration setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95de55e",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "Load and examine the raw case data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c96bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_cases(data_dir):\n",
    "    \"\"\"Load all case files from directory.\"\"\"\n",
    "    cases = []\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    for json_file in data_path.rglob(\"*.json\"):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                case_data = json.load(f)\n",
    "                case_data['source_file'] = str(json_file)\n",
    "                case_data['amendment_type'] = 'First Amendment' if 'first_amendment' in str(json_file) else 'Fourth Amendment'\n",
    "                cases.append(case_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return cases\n",
    "\n",
    "# Load all cases\n",
    "raw_cases = load_all_cases(\"../data/raw\")\n",
    "print(f\"Loaded {len(raw_cases)} cases\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(raw_cases)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5975e8",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "Get basic statistics about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce853639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amendment distribution\n",
    "amendment_counts = df['amendment_type'].value_counts()\n",
    "print(\"Amendment Distribution:\")\n",
    "print(amendment_counts)\n",
    "\n",
    "# Plot amendment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "amendment_counts.plot(kind='bar')\n",
    "plt.title('Cases by Amendment Type')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Text length analysis\n",
    "df['facts_length'] = df['facts_of_the_case'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
    "df['question_length'] = df['question'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
    "df['conclusion_length'] = df['conclusion'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
    "\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"Facts - Mean: {df['facts_length'].mean():.1f}, Median: {df['facts_length'].median():.1f}\")\n",
    "print(f\"Questions - Mean: {df['question_length'].mean():.1f}, Median: {df['question_length'].median():.1f}\")\n",
    "print(f\"Conclusions - Mean: {df['conclusion_length'].mean():.1f}, Median: {df['conclusion_length'].median():.1f}\")\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['facts_length'], bins=20, alpha=0.7, label='Facts')\n",
    "plt.hist(df['conclusion_length'], bins=20, alpha=0.7, label='Conclusions')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0478807",
   "metadata": {},
   "source": [
    "## Content Analysis\n",
    "\n",
    "Analyze the content of the cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years from case names\n",
    "def extract_year(case_name):\n",
    "    \"\"\"Extract year from case name.\"\"\"\n",
    "    if pd.isna(case_name):\n",
    "        return None\n",
    "    \n",
    "    # Look for 4-digit year\n",
    "    year_match = re.search(r'(19|20)\\d{2}', str(case_name))\n",
    "    if year_match:\n",
    "        return int(year_match.group())\n",
    "    \n",
    "    # Look for year ranges like 1900-1940\n",
    "    range_match = re.search(r'(19|20)\\d{2}-(19|20)\\d{2}', str(case_name))\n",
    "    if range_match:\n",
    "        start_year = int(range_match.group().split('-')[0])\n",
    "        return start_year\n",
    "    \n",
    "    return None\n",
    "\n",
    "df['year'] = df['name'].apply(extract_year)\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "print(f\"Cases by decade:\")\n",
    "df['decade'] = (df['year'] // 10) * 10\n",
    "decade_counts = df['decade'].value_counts().sort_index()\n",
    "print(decade_counts.head(10))\n",
    "\n",
    "# Plot cases by decade\n",
    "plt.figure(figsize=(12, 6))\n",
    "decade_counts.plot(kind='bar')\n",
    "plt.title('Constitutional Law Cases by Decade')\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c16e7",
   "metadata": {},
   "source": [
    "## Word Analysis\n",
    "\n",
    "Analyze common terms and concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all text for analysis\n",
    "all_text = \"\"\n",
    "for _, row in df.iterrows():\n",
    "    facts = str(row['facts_of_the_case']) if pd.notna(row['facts_of_the_case']) else \"\"\n",
    "    conclusion = str(row['conclusion']) if pd.notna(row['conclusion']) else \"\"\n",
    "    all_text += f\" {facts} {conclusion}\"\n",
    "\n",
    "# Clean text for analysis\n",
    "def clean_text_for_analysis(text):\n",
    "    \"\"\"Clean text for word analysis.\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned_text = clean_text_for_analysis(all_text)\n",
    "\n",
    "# Get word frequencies\n",
    "words = cleaned_text.split()\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Remove common stop words\n",
    "stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'a', 'an', 'was', 'is', 'are', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall', 'that', 'this', 'these', 'those', 'they', 'them', 'their', 'there', 'where', 'when', 'what', 'who', 'which', 'how', 'why', 'if', 'as', 'so', 'not', 'no', 'all', 'any', 'some', 'each', 'every', 'other', 'another', 'such', 'than', 'only', 'own', 'same', 'few', 'more', 'most', 'less', 'much', 'many', 'little', 'large', 'small', 'good', 'bad', 'new', 'old', 'long', 'short', 'high', 'low', 'big', 'small', 'right', 'left', 'up', 'down', 'out', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'should', 'now'}\n",
    "filtered_freq = {word: count for word, count in word_freq.items() if word not in stop_words and len(word) > 3}\n",
    "\n",
    "# Top 20 most common words\n",
    "top_words = dict(sorted(filtered_freq.items(), key=lambda x: x[1], reverse=True)[:20])\n",
    "\n",
    "print(\"Top 20 most common words:\")\n",
    "for word, count in top_words.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Plot word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "words_list = list(top_words.keys())\n",
    "counts_list = list(top_words.values())\n",
    "plt.bar(words_list, counts_list)\n",
    "plt.title('Top 20 Most Common Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffde846",
   "metadata": {},
   "source": [
    "## Legal Concepts Analysis\n",
    "\n",
    "Analyze constitutional and legal concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define legal concept categories\n",
    "legal_concepts = {\n",
    "    'First Amendment': ['speech', 'religion', 'press', 'assembly', 'petition', 'expression', 'establishment', 'exercise'],\n",
    "    'Fourth Amendment': ['search', 'seizure', 'warrant', 'privacy', 'probable', 'reasonable', 'unreasonable'],\n",
    "    'Court Actions': ['held', 'ruled', 'decided', 'concluded', 'found', 'determined', 'affirmed', 'reversed'],\n",
    "    'Legal Standards': ['constitutional', 'unconstitutional', 'violated', 'protected', 'prohibited', 'permitted', 'compelling', 'strict'],\n",
    "    'Parties': ['plaintiff', 'defendant', 'petitioner', 'respondent', 'appellant', 'appellee', 'government', 'state']\n",
    "}\n",
    "\n",
    "# Count occurrences of each concept category\n",
    "concept_counts = {}\n",
    "for category, terms in legal_concepts.items():\n",
    "    count = sum(filtered_freq.get(term, 0) for term in terms)\n",
    "    concept_counts[category] = count\n",
    "\n",
    "print(\"Legal Concept Frequencies:\")\n",
    "for category, count in concept_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "# Plot concept frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "categories = list(concept_counts.keys())\n",
    "counts = list(concept_counts.values())\n",
    "plt.bar(categories, counts)\n",
    "plt.title('Legal Concept Frequencies')\n",
    "plt.xlabel('Concept Categories')\n",
    "plt.ylabel('Total Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c75a6f1",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "Check for data quality issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(\"Missing Data Analysis:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "\n",
    "# Check for empty strings\n",
    "print(\"\\nEmpty String Analysis:\")\n",
    "empty_facts = df['facts_of_the_case'].apply(lambda x: len(str(x).strip()) == 0 if pd.notna(x) else True).sum()\n",
    "empty_questions = df['question'].apply(lambda x: len(str(x).strip()) == 0 if pd.notna(x) else True).sum()\n",
    "empty_conclusions = df['conclusion'].apply(lambda x: len(str(x).strip()) == 0 if pd.notna(x) else True).sum()\n",
    "\n",
    "print(f\"Empty facts: {empty_facts}\")\n",
    "print(f\"Empty questions: {empty_questions}\")\n",
    "print(f\"Empty conclusions: {empty_conclusions}\")\n",
    "\n",
    "# Check for very short/long texts\n",
    "print(\"\\nText Length Issues:\")\n",
    "short_facts = (df['facts_length'] < 20).sum()\n",
    "long_facts = (df['facts_length'] > 200).sum()\n",
    "short_conclusions = (df['conclusion_length'] < 20).sum()\n",
    "long_conclusions = (df['conclusion_length'] > 300).sum()\n",
    "\n",
    "print(f\"Very short facts (<20 words): {short_facts}\")\n",
    "print(f\"Very long facts (>200 words): {long_facts}\")\n",
    "print(f\"Very short conclusions (<20 words): {short_conclusions}\")\n",
    "print(f\"Very long conclusions (>300 words): {long_conclusions}\")\n",
    "\n",
    "# Show examples of problematic cases\n",
    "print(\"\\nExamples of very short conclusions:\")\n",
    "short_conclusion_cases = df[df['conclusion_length'] < 20]\n",
    "for _, case in short_conclusion_cases.head(3).iterrows():\n",
    "    print(f\"Case: {case['name']}\")\n",
    "    print(f\"Conclusion: {case['conclusion']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66d473",
   "metadata": {},
   "source": [
    "## Processed Data Analysis\n",
    "\n",
    "Analyze the processed training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688250d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data if available\n",
    "processed_train_file = \"../data/processed/train_cleaned.jsonl\"\n",
    "processed_val_file = \"../data/processed/validation_cleaned.jsonl\"\n",
    "\n",
    "if os.path.exists(processed_train_file):\n",
    "    # Load processed training data\n",
    "    train_data = []\n",
    "    with open(processed_train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            train_data.append(json.loads(line))\n",
    "    \n",
    "    # Load processed validation data\n",
    "    val_data = []\n",
    "    with open(processed_val_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            val_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"Processed training examples: {len(train_data)}\")\n",
    "    print(f\"Processed validation examples: {len(val_data)}\")\n",
    "    \n",
    "    # Analyze instruction and response lengths\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df['instruction_length'] = train_df['instruction'].apply(lambda x: len(x.split()))\n",
    "    train_df['response_length'] = train_df['response'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    print(f\"\\nProcessed Data Statistics:\")\n",
    "    print(f\"Instruction length - Mean: {train_df['instruction_length'].mean():.1f}, Median: {train_df['instruction_length'].median():.1f}\")\n",
    "    print(f\"Response length - Mean: {train_df['response_length'].mean():.1f}, Median: {train_df['response_length'].median():.1f}\")\n",
    "    \n",
    "    # Plot processed data lengths\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(train_df['instruction_length'], bins=20, alpha=0.7)\n",
    "    plt.title('Instruction Length Distribution')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(train_df['response_length'], bins=20, alpha=0.7)\n",
    "    plt.title('Response Length Distribution')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show example processed data\n",
    "    print(\"\\nExample processed training instance:\")\n",
    "    example = train_data[0]\n",
    "    print(f\"Case: {example['name']}\")\n",
    "    print(f\"Instruction: {example['instruction'][:200]}...\")\n",
    "    print(f\"Response: {example['response'][:200]}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"Processed data not found. Run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68ac28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data exploration summary and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a590895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA EXPLORATION SUMMARY ===\")\n",
    "print(f\"Total cases loaded: {len(df)}\")\n",
    "print(f\"First Amendment cases: {amendment_counts.get('First Amendment', 0)}\")\n",
    "print(f\"Fourth Amendment cases: {amendment_counts.get('Fourth Amendment', 0)}\")\n",
    "print(f\"Time span: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"Average facts length: {df['facts_length'].mean():.1f} words\")\n",
    "print(f\"Average conclusion length: {df['conclusion_length'].mean():.1f} words\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. Data quality is generally good with comprehensive case coverage\")\n",
    "print(\"2. Both amendment types are well represented\")\n",
    "print(\"3. Text lengths are appropriate for training\")\n",
    "print(\"4. Consider additional preprocessing for HTML tags and citations\")\n",
    "print(\"5. Monitor for potential overfitting on specific time periods\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Proceed with model training using the processed data\")\n",
    "print(\"2. Consider data augmentation if needed\")\n",
    "print(\"3. Evaluate model performance on both amendment types\")\n",
    "print(\"4. Monitor for bias towards specific legal concepts\")\n",
    "\n",
    "print(\"\\nData exploration completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
